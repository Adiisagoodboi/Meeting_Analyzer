{"audio_file": "sample_meeting_1.wav", "stages": {"diarization": {"returncode": 0, "stdout": "[pyannote] loading diarization pipeline (this may take a minute first time)...\nModel was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.8.0+cpu. Bad things might happen unless you revert torch to 1.x.\n[diarize] processing D:\\meeting-analyzer\\audio\\sample_meeting_1.wav ...\n[diarize] saved: results\\sample_meeting_1_diarization.csv\n\n=== DIARIZATION SUMMARY ===\n{\n  \"stem\": \"sample_meeting_1\",\n  \"diar_csv\": \"results\\\\sample_meeting_1_diarization.csv\",\n  \"segments_count\": 39\n}\n{\"stem\": \"sample_meeting_1\", \"diar_csv\": \"results\\\\sample_meeting_1_diarization.csv\", \"segments_count\": 39}\n", "stderr": "D:\\meeting-analyzer\\.venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  torchaudio.list_audio_backends()\nD:\\meeting-analyzer\\.venv\\Lib\\site-packages\\speechbrain\\utils\\torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  available_backends = torchaudio.list_audio_backends()\nLightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\Aditya Nair\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\nD:\\meeting-analyzer\\.venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  torchaudio.list_audio_backends()\nD:\\meeting-analyzer\\.venv\\Lib\\site-packages\\speechbrain\\utils\\autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\nD:\\meeting-analyzer\\.venv\\Lib\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n  warnings.warn(\nD:\\meeting-analyzer\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n  warnings.warn(\nD:\\meeting-analyzer\\.venv\\Lib\\site-packages\\pyannote\\audio\\core\\io.py:85: UserWarning: torchaudio._backend.utils.info has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  info = torchaudio.info(file[\"audio\"], backend=backend)\nD:\\meeting-analyzer\\.venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:120: UserWarning: torchaudio._backend.common.AudioMetaData has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n  return AudioMetaData(\n"}, "asr": {"returncode": 0, "stdout": "[whisper] loading model 'small' (this may take a while)...\n[whisper] transcribing D:\\meeting-analyzer\\audio\\sample_meeting_1.wav ...\nDetected language: English\n[whisper] saved transcript -> results\\sample_meeting_1_transcript.txt\n[whisper] saved segments -> results\\sample_meeting_1_asr_segments.json\n\n=== ASR SUMMARY ===\n{\n  \"stem\": \"sample_meeting_1\",\n  \"transcript_txt\": \"results\\\\sample_meeting_1_transcript.txt\",\n  \"segments_json\": \"results\\\\sample_meeting_1_asr_segments.json\",\n  \"segments_count\": 77\n}\n{\"stem\": \"sample_meeting_1\", \"transcript_txt\": \"results\\\\sample_meeting_1_transcript.txt\", \"segments_json\": \"results\\\\sample_meeting_1_asr_segments.json\", \"segments_count\": 77}\n", "stderr": "D:\\meeting-analyzer\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n\n  0%|          | 0/25027 [00:00<?, ?frames/s]\n 11%|#1        | 2800/25027 [00:08<01:07, 330.92frames/s]\n 23%|##2       | 5700/25027 [00:16<00:54, 354.73frames/s]\n 34%|###4      | 8600/25027 [00:24<00:46, 355.03frames/s]\n 45%|####5     | 11300/25027 [00:32<00:40, 338.64frames/s]\n 56%|#####5    | 14000/25027 [00:40<00:31, 347.58frames/s]\n 68%|######7   | 16900/25027 [00:47<00:22, 362.60frames/s]\n 79%|#######8  | 19700/25027 [00:55<00:14, 361.52frames/s]\n 90%|########9 | 22400/25027 [01:02<00:07, 363.71frames/s]\n100%|##########| 25027/25027 [01:09<00:00, 367.34frames/s]\n100%|##########| 25027/25027 [01:09<00:00, 358.44frames/s]\n"}, "mapping": {"returncode": 0, "stdout": "[info] wrote metrics: results\\sample_meeting_3_metrics.csv (mapped 4 speakers)\n[info] mapping.json already exists; not overwriting. Suggested mapping:\n{\n  \"SPEAKER_02\": \"Anita\",\n  \"SPEAKER_01\": \"Rahul\",\n  \"SPEAKER_03\": \"Priya\",\n  \"SPEAKER_00\": \"Karen\"\n}\n\n=== SUMMARY ===\n{\n  \"stem\": \"sample_meeting_3\",\n  \"suggested_mapping\": {\n    \"SPEAKER_02\": \"Anita\",\n    \"SPEAKER_01\": \"Rahul\",\n    \"SPEAKER_03\": \"Priya\",\n    \"SPEAKER_00\": \"Karen\"\n  },\n  \"durations_seconds\": {\n    \"Team\": 35.13,\n    \"Slightly\": 26.68,\n    \"Still\": 60.7,\n    \"Priya\": 23.3\n  }\n}\n", "stderr": ""}}}
